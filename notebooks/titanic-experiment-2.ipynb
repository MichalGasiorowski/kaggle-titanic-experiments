{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.4\r\n"
     ]
    }
   ],
   "source": [
    "from copyreg import pickle\n",
    "!python -V"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MLflow setup:\n",
    "\n",
    "To run this example you need to launch the mlflow server locally by running the following command in your terminal:\n",
    "\n",
    "`mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0 --port 5000`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To start MLflow Tracking Server enabled with proxied artifact storage access:\n",
    "( see more in : [https://www.mlflow.org/docs/latest/tracking.html#where-runs-are-recorded](https://www.mlflow.org/docs/latest/tracking.html#where-runs-are-recorded) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'mlflow server --host 0.0.0.0 --port 5000 --serve-artifacts --artifacts-destination s3://mlflow-enkidupal-experiments/'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3_BUCKET = 's3://mlflow-enkidupal-experiments'\n",
    "f'mlflow server --host 0.0.0.0 --port 5000 --serve-artifacts --artifacts-destination {S3_BUCKET}/'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To rerun the server, this command has to be run in *correct* directory,\n",
    "where previous mlflow server was run, to point backend-store-uri to mlflow.db !\n",
    "\n",
    "`cd project_root`\n",
    "\n",
    "\n",
    " `mlflow server --host 0.0.0.0 --port 5000 --serve-artifacts --artifacts-destination s3://mlflow-enkidupal-experiments/`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from toolz import compose\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "backend_inline.set_matplotlib_formats('svg')\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "_data_root = os.path.join(\"./../\", 'data')\n",
    "_data_root_raw = os.path.join(_data_root, 'raw')\n",
    "_data_root_processed =  os.path.join(_data_root, 'processed')\n",
    "\n",
    "_train_dirpath = os.path.join(_data_root_raw, \"train\")\n",
    "_train_filepath = os.path.join(_train_dirpath, \"train.csv\")\n",
    "_test_dirpath = os.path.join(_data_root_raw, \"test\")\n",
    "_test_filepath = os.path.join(_test_dirpath, \"test.csv\")\n",
    "\n",
    "_train_processed_dirpath = os.path.join(_data_root_processed, \"train\")\n",
    "_train_processed_filepath = os.path.join(_data_root_processed, \"train.csv\")\n",
    "_valid_processed_dirpath = os.path.join(_data_root_processed, \"valid\")\n",
    "_valid_processed_filepath = os.path.join(_data_root_processed, \"valid.csv\")\n",
    "_test_processed_dirpath = os.path.join(_data_root_processed, \"test\")\n",
    "_test_processed_filepath = os.path.join(_data_root_processed, \"test.csv\")\n",
    "\n",
    "os.makedirs(_train_dirpath, exist_ok=True)\n",
    "os.makedirs(_test_dirpath, exist_ok=True)\n",
    "os.makedirs(_train_processed_dirpath, exist_ok=True)\n",
    "os.makedirs(_valid_processed_dirpath, exist_ok=True)\n",
    "os.makedirs(_test_processed_dirpath, exist_ok=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download data from kaggle, unzip it and copy it to data folder\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    !kaggle competitions download -c titanic -p {_data_root} --force\n",
    "    !unzip -o {_data_root}/\"titanic.zip\" -d {_data_root}\n",
    "    !cp {_data_root}/\"train.csv\" {_train_filepath}\n",
    "    !cp {_data_root}/\"test.csv\" {_test_filepath}\n",
    "\n",
    "    # clean up\n",
    "    !rm  {_data_root}/*.csv  {_data_root}/*.zip\n",
    "\n",
    "def extract_target(data: pd.DataFrame, target=\"Survived\"):\n",
    "    targets = data[target].values\n",
    "    return targets\n",
    "\n",
    "def preprocess_df_2(df: pd.DataFrame, transforms, categorical, numerical):\n",
    "    \"\"\"Return processed features dict and target.\"\"\"\n",
    "\n",
    "    # Apply in-between transformations\n",
    "    df = compose(*transforms[::-1])(df)\n",
    "\n",
    "    # For dict vectorizer: int = ignored, str = one-hot\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame, transforms, categorical, numerical):\n",
    "    \"\"\"Return processed features dict and target.\"\"\"\n",
    "\n",
    "    # Apply in-between transformations\n",
    "    df = compose(*transforms[::-1])(df)\n",
    "\n",
    "    # For dict vectorizer: int = ignored, str = one-hot\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    # Convert dataframe to feature dictionaries\n",
    "    feature_dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    return feature_dicts\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Return processed features dict and target.\"\"\"\n",
    "\n",
    "    # Load dataset\n",
    "    if filename.endswith('parquet'):\n",
    "        df = pd.read_parquet(filename)\n",
    "    elif filename.endswith('csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "    else:\n",
    "        raise \"Error: not supported file format.\"\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_no_extract_target(filename, transforms, categorical, numerical):\n",
    "    df = read_data(filename)\n",
    "\n",
    "    feature_dicts = preprocess_df(df, transforms, categorical, numerical)\n",
    "\n",
    "    return feature_dicts\n",
    "\n",
    "def split_train_read(filename: str, val_size=0.2, random_state=42):\n",
    "    df_train_full = read_data(filename)\n",
    "\n",
    "    df_train, df_val = train_test_split(df_train_full, test_size=val_size, random_state=random_state)\n",
    "    return df_train, df_val\n",
    "\n",
    "def save_preprocessed(df: pd.DataFrame, path):\n",
    "    df.to_csv(path)\n",
    "\n",
    "def dump_pickle(obj, filename):\n",
    "    with open(filename, \"wb\") as f_out:\n",
    "        return pickle.dump(obj, f_out)\n",
    "\n",
    "def run():\n",
    "    mlflow.set_tracking_uri(\"http://0.0.0.0:5000\")\n",
    "    mlflow.set_experiment(\"titanic-experiment\")\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        download_data()\n",
    "        transforms = []\n",
    "        target = 'Survived'\n",
    "        categorical = ['Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch']\n",
    "        numerical = ['Fare']\n",
    "\n",
    "        df_train, df_val = split_train_read(_train_filepath, val_size=0.2, random_state=42)\n",
    "\n",
    "        train_dicts, y_train = preprocess_df(df_train, transforms, categorical, numerical), extract_target(df_train)\n",
    "        val_dicts, y_val = preprocess_df(df_val, transforms, categorical, numerical), extract_target(df_val)\n",
    "\n",
    "        df_test = read_data(_test_filepath)\n",
    "        test_dicts = preprocess_df(df_test, transforms, categorical, numerical)\n",
    "\n",
    "        # Fit all possible categories\n",
    "        dv = DictVectorizer()\n",
    "        dv.fit(train_dicts)\n",
    "\n",
    "        X_train = dv.transform(train_dicts)\n",
    "        X_val = dv.transform(val_dicts)\n",
    "        X_test = dv.transform(test_dicts)\n",
    "\n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        with open(\"../models/preprocessor.b\", \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "        mlflow.log_artifact(\"../models/preprocessor.b\", artifact_path=\"preprocessor\")\n",
    "\n",
    "        #    mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")\n",
    "        #mlflow.log_artifact(local_path=os.path.join(\"../models\", \"rfc.pkl\"), artifact_path=\"models_pickle\")\n",
    "        mlflow.sklearn.log_model(model, artifact_path=\"models_pickle\")\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        accuracy = np.round(accuracy_score(y_val, y_pred), 4)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        print(accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "transforms = []\n",
    "target = 'Survived'\n",
    "categorical = ['Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch']\n",
    "numerical = ['Fare']\n",
    "\n",
    "df_train, df_val = split_train_read(_train_filepath, val_size=0.2, random_state=42)\n",
    "df_train_preprocessed = preprocess_df_2(df_train, transforms, categorical, numerical)\n",
    "\n",
    "df_train_preprocessed = df_train_preprocessed.reset_index()\n",
    "#df_train_preprocessed[categorical].head()\n",
    "\n",
    "#df_train_preprocessed[categorical].isna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "{'Sex': {0: 'male'},\n 'Pclass': {0: '3'},\n 'Embarked': {0: 'Q'},\n 'SibSp': {0: '0'},\n 'Parch': {0: '0'}}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_preprocessed.head()\n",
    "\n",
    "df_train_preprocessed[categorical][0:1].to_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading titanic.zip to ./../data\r\n",
      "  0%|                                               | 0.00/34.1k [00:00<?, ?B/s]\r\n",
      "100%|██████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 1.82MB/s]\r\n",
      "Archive:  ./../data/titanic.zip\r\n",
      "  inflating: ./../data/gender_submission.csv  \r\n",
      "  inflating: ./../data/test.csv      \r\n",
      "  inflating: ./../data/train.csv     \r\n",
      "0.8045\n"
     ]
    }
   ],
   "source": [
    "run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow.pyfunc.loaded_model:\n",
      "  artifact_path: models_pickle\n",
      "  flavor: mlflow.xgboost\n",
      "  run_id: d6dc868dda9e41ecaf536a91f41e059e\n",
      "\n",
      "Pipeline(steps=[('dictvectorizer', DictVectorizer())])\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "\n",
    "import src.features.build_features\n",
    "\n",
    "RUN_ID = 'd6dc868dda9e41ecaf536a91f41e059e'\n",
    "\n",
    "logged_model = f's3://mlflow-enkidupal-experiments/1/{RUN_ID}/artifacts/models_pickle/'\n",
    "\n",
    "model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "s3client = boto3.client('s3' )\n",
    "\n",
    "response = s3client.get_object(Bucket='mlflow-enkidupal-experiments', Key=f'1/{RUN_ID}/artifacts/preprocessor/preprocessor.b')\n",
    "\n",
    "body = response['Body'].read()\n",
    "preprocessor = pickle.loads(body)\n",
    "\n",
    "print(model)\n",
    "print(preprocessor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep:    (0, 4)\t69.34\n",
      "  (0, 5)\t10.0\n",
      "  (0, 6)\t0.0\n",
      "  (0, 7)\t1.0\n",
      "  (0, 9)\t1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8314826"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.features.build_features import preprocess_test\n",
    "\n",
    "passenger_X = {\n",
    "    \"Sex\": ['female'],\n",
    "    \"Pclass\": [0],\n",
    "    \"Embarked\": ['C'],\n",
    "    \"SibSp\": [1],\n",
    "    \"Parch\": [10],\n",
    "    \"Fare\": [69.34]\n",
    "}\n",
    "\n",
    "df_X = pd.DataFrame.from_dict(passenger_X)\n",
    "\n",
    "\n",
    "prep = preprocess_test(df_X, preprocessor )\n",
    "print('prep: ', prep)\n",
    "\n",
    "prediction = model.predict(prep)\n",
    "prediction[0]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> int64\n",
      "<class 'numpy.ndarray'> int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8328651685393258"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_train_full = read_data(_train_filepath)\n",
    "df_train_full.head()\n",
    "\n",
    "\n",
    "targets = extract_target(df_train_full)\n",
    "#targets = list(targets)\n",
    "print(type(targets), targets.dtype)\n",
    "prep = preprocess_test(df_train_full, preprocessor)\n",
    "\n",
    "y_pred = model.predict(prep)\n",
    "predictions = np.array([round(value) for value in y_pred])\n",
    "print(type(predictions), predictions.dtype)\n",
    "acc = accuracy_score(targets, y_pred.round())\n",
    "acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "two_passengers = [\n",
    "    {\n",
    "        \"Sex\": ['female'],\n",
    "        \"Pclass\": [0],\n",
    "        \"Embarked\": ['C'],\n",
    "        \"SibSp\": [1],\n",
    "        \"Parch\": [10],\n",
    "        \"Fare\": [69.34]\n",
    "    },\n",
    "    {\n",
    "        \"Sex\": ['male'],\n",
    "        \"Pclass\": [1],\n",
    "        \"Embarked\": ['C'],\n",
    "        \"SibSp\": [0],\n",
    "        \"Parch\": [2],\n",
    "        \"Fare\": [8]\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.json_normalize(two_passengers, record_path='location', meta=['id', 'name'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "['Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch', 'Fare']"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical = ['Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch']\n",
    "numerical = ['Fare']\n",
    "\n",
    "all_columns = categorical + numerical\n",
    "all_columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "[0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0]"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "(numpy.ndarray, list)"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(targets), type(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}